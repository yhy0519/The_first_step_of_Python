{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python15_12_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1laNlIaqXnYGi7Gbq9Msilt0z3wsrduqE",
      "authorship_tag": "ABX9TyPMbn7XMNRYRkl0bXE5e+iP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yhy0519/The_first_step_of_Python/blob/main/python15_12_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG0XNvP3Yv3l"
      },
      "source": [
        "** 빅데이터 수집을 위해 반드시 알아야할 필수 기술 ?  \r\n",
        "\r\n",
        "    웹스크롤링\r\n",
        "\r\n",
        "** 웹스크롤링을 위해서 사용한 파이썬 모듈 ?  \r\n",
        "\r\n",
        "    beautiful soup 모듈\r\n",
        "\r\n",
        "** 웹스크롤링을 편하게 잘 하기 위한 비법 ?\r\n",
        "\r\n",
        "    크롬의 개발자 모드(F12) 로 들어가서 화살표를 이용해서 웹상에서 스크롤링하기 원하는 지점의 html 클래스 이름을 빠르게 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaDwjgvnY1S0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBpkIhIFY1Lq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH2rToK4Y1zT"
      },
      "source": [
        "**■ 142. 웹스크롤링 실전 1단계 (ebs 레이디 버그 게시판)**\r\n",
        "\r\n",
        "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106 - 시청자게시판\r\n",
        "\r\n",
        "우리회사의 신제품이 출시 되었을때 그 제품에 대한 사람들의 반을을 데이터 분석을 하고자 할 때\r\n",
        "웹스크롤링 + 데이터 시각화를 이용하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZwglQVtY8Wv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg1WQklzagmk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVJ2aLnYY8Rb"
      },
      "source": [
        "예제1. ebs 레이디버그 시청자 게시판의 url 을 가지고 직접 html 문서를 내려 받을 수 있도록 코드를 구현\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGWoBZgjZC_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a7d760-85de-4886-d612-7c7ecd8c3518"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request       # 웹상의 url 을 파이썬이 인식할 수 있도록 해주는 모듈\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)        # 사람이 알아볼 수 있는 위의 url 을 파이썬이 알아볼 수 있도록 변환 첫번째 작업\r\n",
        "print(url)\r\n",
        "\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")   # 위의 url 의 html 문서들을 result 변수에 담는 두번째 작업\r\n",
        "#print(result)               # 위의 url 의 html 전체 문서가 다 출력되고 있음."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<urllib.request.Request object at 0x7fc53c3d2320>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ddcYFlY8J0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "injX29etZP0z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa7-5hZlZPv8"
      },
      "source": [
        "예제2. 위에서 긁어온 html 코드를 Beautiful soup 의 함수를 이용해서 웹스크롤링을\r\n",
        "\t  할 수 있도록 Beautiful soup 를 쓸 수 있게 파싱하시오\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-X2siCHYYq5"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "print(soup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-WacqFtZ9JM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FaemfZZ9GA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWXOhxibZ9Cd"
      },
      "source": [
        "예제3. 지금 페이지의 시청자 게시판의 글 내용에 해당하는 부분의 태그와 클래스 이름을 알아내시오\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ58AFfrZ9zk"
      },
      "source": [
        "    <p class=\"con\"> 레이디버그 찐 팬이에요 꿈에도 나왔어요 </p>\r\n",
        "    <p class=\"con\"> 최고의 히어로 만화에요. </p>\r\n",
        "    <p class=\"con\"> 재미있 다 </p>\r\n",
        "\r\n",
        "시청자 게시판 글 내용 class 이름: con"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0hYHM64afpm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad41cLmEaflk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZtP7BU6afhk"
      },
      "source": [
        "예제4. p 태그중에 class 가 con 에 해당하는 부분을 스크롤링 하시오 !\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EiAxWkGaEly"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "result2 = soup.find_all( 'p', class_='con')\r\n",
        "print(result2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ktXTuT-ao1Q"
      },
      "source": [
        "설명: find 함수는 맨 처음 하나만 가져오는데 find_all 은 p 태그에 class 이름 con 에 해당하는 부분을 모두 가져온다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po7T5uaiau6Y"
      },
      "source": [
        "결과:  list 로 결과값 출력\r\n",
        "\r\n",
        "    [[<p class=\"con\"> 레이디버그 찐 팬이에요 꿈에도 나왔어요 </p>, \r\n",
        "    <p class=\"con\"> 최고의 히어로 만화에요. </p>,\r\n",
        "    <p class=\"con\"> 재미있 다, … ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl6MJCC7ayDl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ao0gpUax-m"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBwjA78sax4D"
      },
      "source": [
        "예제5. 위의 결과에서 html 문서 말고 한글 텍스트만 가져오시오 ! (위의 result2 는 리스트 입니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVhNsRdra0GQ"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "result2 = soup.find_all( 'p', class_='con')\r\n",
        "for i in result2:\r\n",
        "    print( i.get_text() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjHgMxgDbJOs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzexTM9JbJH8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQKCOYr1bJu7"
      },
      "source": [
        "예제6. 위에서 출력되고 있는 텍스트들이 좀 더 깔끔하게 나오게 하시오 ! (양옆에 공백 포함)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSBHlu6lbK7V",
        "outputId": "3e12d795-c53e-4899-f9a1-9f2ab44cd3b1"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "result2 = soup.find_all( 'p', class_='con')\r\n",
        "for i in result2:\r\n",
        "    print( i.get_text(\" \", strip = True) )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "레이디버그 찐 팬이에요 \r\n",
            "꿈에도 나왔어요\n",
            "최고의 히어로 만화에요.\n",
            "재미있 다\n",
            "어제 흐름상 종영 같지 않았는데 갑자기 끝이네요??\n",
            "〈미라큘러스: 레이디버그와 블랙캣〉종영 안내!\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "〈미라큘러스: 레이디버그와 블랙캣〉-시즌3가 \r\n",
            "\r\n",
            "방송 종료되었습니다, 그동안 시청해주셔서 감사드립니다!\r\n",
            "\r\n",
            "2021년 새로운 시즌으로 여러분을 찾아뵙도록 노력하겠습니다!\r\n",
            "\r\n",
            "고맙습니다~\n",
            "아이들 때문에 보게 되었는데 어른인 제가 더 좋아하는 프로그램이 되었네요 아이들이랑 볼 때 너무 신나요 함께 볼 수 있는 보기 드문 애니매이션이라 더 좋네요 시즌 4도 너무 너무 기대되고 기다리고 있습니다\n",
            "2021년이 빨리되서 레이디버그 시즌4를 보면 좋겠네요~\n",
            "레이디버그... 제가 어렸을때도 너~~무 좋아 하던 방송 이에요!!!!!!!\n",
            "이렇게 재미있는프로는 이 세상에 하나뿐이예요 아 그리고 뉴욕편은 언제 올려주시나요?\n",
            "너모 재밌어요!!! 저 시즌 1부터 다 본방으로 챙겨본 미라큘러 입니다ㅋㅋㅋ 진짜  재밌구요 팬카페랑 이런데도 다 가입해있어요... 방도 레이디버그 사진으로 덕-질☆\n",
            "완전 재밌다\n",
            "근뎅.....오즈 긑나서....레이디버그가 하는거잖아여....ㅠ\n",
            "우왕!\n",
            "넘재밌어요.매일하면 좋겠어요\n",
            "재밌어요 완전 꿀잼이요 사랑합니다\n",
            "ㅎㅎ\n",
            "진짜여???\n",
            "보니하니,레이디버그,형사 가제트,오즈,삐삐에 있었어요!\n",
            "오늘부터 미라큘러스 시즌3 다시 방영한다네요!^^\r\n",
            "목,금 저녁 7시\n",
            "전 세매큐,픽시,가제트형사,오:마법을 찾아서,히어로써클,로빈후드에 있어여!ㅎㅎ즈\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ0lJTaGbR-7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx8pWylybR4M"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eiAQpdWbRn0"
      },
      "source": [
        "예제7. 위에서 출력되고 있는 텍스트들이 params1 라는 비어있는 리스트에 담기게 하시오 !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puQ3Hg8_bVHr",
        "outputId": "85e7907f-b625-4f40-9fe8-30c4b6723215"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "result2 = soup.find_all( 'p', class_='con')\r\n",
        "params1 = []\r\n",
        "for i in result2:\r\n",
        "    params1.append( i.get_text(\" \", strip = True) )\r\n",
        "print(params1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['레이디버그 찐 팬이에요 \\r\\n꿈에도 나왔어요', '최고의 히어로 만화에요.', '재미있 다', '어제 흐름상 종영 같지 않았는데 갑자기 끝이네요??', '〈미라큘러스: 레이디버그와 블랙캣〉종영 안내!\\r\\n\\r\\n\\r\\n\\r\\n〈미라큘러스: 레이디버그와 블랙캣〉-시즌3가 \\r\\n\\r\\n방송 종료되었습니다, 그동안 시청해주셔서 감사드립니다!\\r\\n\\r\\n2021년 새로운 시즌으로 여러분을 찾아뵙도록 노력하겠습니다!\\r\\n\\r\\n고맙습니다~', '아이들 때문에 보게 되었는데 어른인 제가 더 좋아하는 프로그램이 되었네요 아이들이랑 볼 때 너무 신나요 함께 볼 수 있는 보기 드문 애니매이션이라 더 좋네요 시즌 4도 너무 너무 기대되고 기다리고 있습니다', '2021년이 빨리되서 레이디버그 시즌4를 보면 좋겠네요~', '레이디버그... 제가 어렸을때도 너~~무 좋아 하던 방송 이에요!!!!!!!', '이렇게 재미있는프로는 이 세상에 하나뿐이예요 아 그리고 뉴욕편은 언제 올려주시나요?', '너모 재밌어요!!! 저 시즌 1부터 다 본방으로 챙겨본 미라큘러 입니다ㅋㅋㅋ 진짜  재밌구요 팬카페랑 이런데도 다 가입해있어요... 방도 레이디버그 사진으로 덕-질☆', '완전 재밌다', '근뎅.....오즈 긑나서....레이디버그가 하는거잖아여....ㅠ', '우왕!', '넘재밌어요.매일하면 좋겠어요', '재밌어요 완전 꿀잼이요 사랑합니다', 'ㅎㅎ', '진짜여???', '보니하니,레이디버그,형사 가제트,오즈,삐삐에 있었어요!', '오늘부터 미라큘러스 시즌3 다시 방영한다네요!^^\\r\\n목,금 저녁 7시', '전 세매큐,픽시,가제트형사,오:마법을 찾아서,히어로써클,로빈후드에 있어여!ㅎㅎ즈']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxucX0dBbX77"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A0I_L-vbX37"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXFHxkmTbXqz"
      },
      "source": [
        "예제8. 게시글을 올린 날짜를 스크롤링하기 위해서 게시글 날짜가 있는 html 문서의\r\n",
        "\t  태그 이름과 클래스 이름을 확인하시오 !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjMw3NAmbYyz"
      },
      "source": [
        "    <span class=\"date\">2021.01.21 11:19</span>\r\n",
        "    <span class=\"date\">2020.12.18 22:33</span>\r\n",
        "    <span class=\"date\">2020.12.11 19:52</span>\r\n",
        "\r\n",
        "태그 이름은 span 이고 클래스 이름은 date 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaBtmmLkbxqk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFcxVvpwbx_V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTqS8dBbbyQT"
      },
      "source": [
        "예제9. 기존 코드에 위의 날짜를 스크롤링하는 코드를 추가하고 위의 날짜를 모두 스크롤링해서 params2 라는 리스트에 담으시오 ~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iduOK6hb2Ve",
        "outputId": "1c8bac41-65c5-4b46-d251-42f184dfc27f"
      },
      "source": [
        "# 1. 웹에서 html 문서를 가져와 beautifulsoup 으로 파싱\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "\r\n",
        "# 2. 시청자 게시판의 날짜와 본문 내용을 가져옵니다.\r\n",
        "\r\n",
        "result1 = soup.find_all( 'span', class_='date')\r\n",
        "result2 = soup.find_all( 'p', class_='con')\r\n",
        "\r\n",
        "\r\n",
        "# 3. 시청자 게시판의 날짜와 본문을 params 와 params2 리스트에 담습니다.\r\n",
        "\r\n",
        "params1 = []\r\n",
        "params2 = []\r\n",
        "for i in result1:\r\n",
        "    params1.append( i.get_text(\" \", strip = True) )\r\n",
        "for i in result2:\r\n",
        "    params2.append( i.get_text(\" \", strip = True) )\r\n",
        "print(params1)\r\n",
        "print(params2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2021.01.21 11:19', '2020.12.18 22:33', '2020.12.11 19:52', '2020.12.11 19:06', '2020.12.11 01:35', '2020.12.10 12:43', '2020.10.17 15:30', '2020.10.16 12:25', '2020.10.15 21:04', '2020.09.29 14:46', '2020.09.29 11:30', '2020.09.21 08:17', '2020.09.21 08:16', '2020.09.20 09:07', '2020.09.19 22:52', '2020.09.16 12:34', '2020.09.14 10:26', '2020.09.11 09:45', '2020.09.11 09:42', '2020.09.06 08:58']\n",
            "['레이디버그 찐 팬이에요 \\r\\n꿈에도 나왔어요', '최고의 히어로 만화에요.', '재미있 다', '어제 흐름상 종영 같지 않았는데 갑자기 끝이네요??', '〈미라큘러스: 레이디버그와 블랙캣〉종영 안내!\\r\\n\\r\\n\\r\\n\\r\\n〈미라큘러스: 레이디버그와 블랙캣〉-시즌3가 \\r\\n\\r\\n방송 종료되었습니다, 그동안 시청해주셔서 감사드립니다!\\r\\n\\r\\n2021년 새로운 시즌으로 여러분을 찾아뵙도록 노력하겠습니다!\\r\\n\\r\\n고맙습니다~', '아이들 때문에 보게 되었는데 어른인 제가 더 좋아하는 프로그램이 되었네요 아이들이랑 볼 때 너무 신나요 함께 볼 수 있는 보기 드문 애니매이션이라 더 좋네요 시즌 4도 너무 너무 기대되고 기다리고 있습니다', '2021년이 빨리되서 레이디버그 시즌4를 보면 좋겠네요~', '레이디버그... 제가 어렸을때도 너~~무 좋아 하던 방송 이에요!!!!!!!', '이렇게 재미있는프로는 이 세상에 하나뿐이예요 아 그리고 뉴욕편은 언제 올려주시나요?', '너모 재밌어요!!! 저 시즌 1부터 다 본방으로 챙겨본 미라큘러 입니다ㅋㅋㅋ 진짜  재밌구요 팬카페랑 이런데도 다 가입해있어요... 방도 레이디버그 사진으로 덕-질☆', '완전 재밌다', '근뎅.....오즈 긑나서....레이디버그가 하는거잖아여....ㅠ', '우왕!', '넘재밌어요.매일하면 좋겠어요', '재밌어요 완전 꿀잼이요 사랑합니다', 'ㅎㅎ', '진짜여???', '보니하니,레이디버그,형사 가제트,오즈,삐삐에 있었어요!', '오늘부터 미라큘러스 시즌3 다시 방영한다네요!^^\\r\\n목,금 저녁 7시', '전 세매큐,픽시,가제트형사,오:마법을 찾아서,히어로써클,로빈후드에 있어여!ㅎㅎ즈']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnjzzaF0cKgK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkOsbNK9cKc0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APmRoAJIcKZs"
      },
      "source": [
        "예제10. 위의 날짜와 본문 내용이 아래와 같이 출력되게 하시오 ! (zip 과 + 를 이용하시오)\r\n",
        "\r\n",
        "    2021.01.21 11:19   레이디버그 찐 팬이에요 \\r\\n꿈에도 나왔어요\r\n",
        "    2020.12.18 22:33   최고의 히어로 만화에요.\r\n",
        "    2020.12.11 19:52   재미있 다\r\n",
        "          :                 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROyWlUIcccfM",
        "outputId": "c0a05aca-f5fb-4b87-a403-6085889471e1"
      },
      "source": [
        "# 1. 웹에서 html 문서를 가져와 beautifulsoup 으로 파싱\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "list_url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "\r\n",
        "# 2. 시청자 게시판의 날짜와 본문 내용을 가져옵니다.\r\n",
        "\r\n",
        "result1 = soup.find_all( 'span', class_='date')\r\n",
        "result2 = soup.find_all( 'p', class_='con')\r\n",
        "\r\n",
        "\r\n",
        "# 3. 시청자 게시판의 날짜와 본문을 params 와 params2 리스트에 담습니다.\r\n",
        "\r\n",
        "params1 = []\r\n",
        "params2 = []\r\n",
        "for i in result1:\r\n",
        "    params1.append( i.get_text(\" \", strip = True) )\r\n",
        "for i in result2:\r\n",
        "    params2.append( i.get_text(\" \", strip = True) )\r\n",
        "\r\n",
        "\r\n",
        "# 4. 날짜와 본문을 같이 출력합니다.\r\n",
        "\r\n",
        "for k, h in zip(params1, params2):\r\n",
        "    print( k + '   ' + h )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021.01.21 11:19   레이디버그 찐 팬이에요 \r\n",
            "꿈에도 나왔어요\n",
            "2020.12.18 22:33   최고의 히어로 만화에요.\n",
            "2020.12.11 19:52   재미있 다\n",
            "2020.12.11 19:06   어제 흐름상 종영 같지 않았는데 갑자기 끝이네요??\n",
            "2020.12.11 01:35   〈미라큘러스: 레이디버그와 블랙캣〉종영 안내!\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "〈미라큘러스: 레이디버그와 블랙캣〉-시즌3가 \r\n",
            "\r\n",
            "방송 종료되었습니다, 그동안 시청해주셔서 감사드립니다!\r\n",
            "\r\n",
            "2021년 새로운 시즌으로 여러분을 찾아뵙도록 노력하겠습니다!\r\n",
            "\r\n",
            "고맙습니다~\n",
            "2020.12.10 12:43   아이들 때문에 보게 되었는데 어른인 제가 더 좋아하는 프로그램이 되었네요 아이들이랑 볼 때 너무 신나요 함께 볼 수 있는 보기 드문 애니매이션이라 더 좋네요 시즌 4도 너무 너무 기대되고 기다리고 있습니다\n",
            "2020.10.17 15:30   2021년이 빨리되서 레이디버그 시즌4를 보면 좋겠네요~\n",
            "2020.10.16 12:25   레이디버그... 제가 어렸을때도 너~~무 좋아 하던 방송 이에요!!!!!!!\n",
            "2020.10.15 21:04   이렇게 재미있는프로는 이 세상에 하나뿐이예요 아 그리고 뉴욕편은 언제 올려주시나요?\n",
            "2020.09.29 14:46   너모 재밌어요!!! 저 시즌 1부터 다 본방으로 챙겨본 미라큘러 입니다ㅋㅋㅋ 진짜  재밌구요 팬카페랑 이런데도 다 가입해있어요... 방도 레이디버그 사진으로 덕-질☆\n",
            "2020.09.29 11:30   완전 재밌다\n",
            "2020.09.21 08:17   근뎅.....오즈 긑나서....레이디버그가 하는거잖아여....ㅠ\n",
            "2020.09.21 08:16   우왕!\n",
            "2020.09.20 09:07   넘재밌어요.매일하면 좋겠어요\n",
            "2020.09.19 22:52   재밌어요 완전 꿀잼이요 사랑합니다\n",
            "2020.09.16 12:34   ㅎㅎ\n",
            "2020.09.14 10:26   진짜여???\n",
            "2020.09.11 09:45   보니하니,레이디버그,형사 가제트,오즈,삐삐에 있었어요!\n",
            "2020.09.11 09:42   오늘부터 미라큘러스 시즌3 다시 방영한다네요!^^\r\n",
            "목,금 저녁 7시\n",
            "2020.09.06 08:58   전 세매큐,픽시,가제트형사,오:마법을 찾아서,히어로써클,로빈후드에 있어여!ㅎㅎ즈\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLX8q3_wchBY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbLJcwq9cg8X"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INq2EBJZcg2k"
      },
      "source": [
        "예제11. 아래의 결과를 출력하는데 페이지 번호가 1번 부터 22번 까지 변경되어서 출력되게하시오 !\r\n",
        "\r\n",
        "    http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\r\n",
        "\r\n",
        "    http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=2&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\r\n",
        "\r\n",
        "    http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=3&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\r\n",
        "\r\n",
        "      :\r\n",
        "\r\n",
        "    http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=22&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqmTEAp2cqSC",
        "outputId": "5d1b27b4-e801-4500-8d4a-28155327e01b"
      },
      "source": [
        "# 위의 url 을 가만히 살펴보면 페이지 번호만 다르고 나머지는 다 같다는 것을 확인할 수 있습니다.\r\n",
        "\r\n",
        "for i in range(1, 23):\r\n",
        "    print('http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page='+ str(i) +'&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=2&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=3&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=4&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=5&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=6&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=7&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=8&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=9&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=10&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=11&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=12&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=13&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=14&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=15&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=16&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=17&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=18&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=19&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=20&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=21&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
            "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=22&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVLbZRiUcpbc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tycXf1Xc1om"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdDZgiPvc1ir"
      },
      "source": [
        "예제12. 예제11번 코드를 예제10번 코드에 적용해서 레이디 버그 전체 게시판의 글들이 날짜와 함께 출력되게하시오\r\n",
        "\r\n",
        "    1. 예제10번 코드를 가져옵니다.\r\n",
        "\r\n",
        "    2. 예제10번 코드에서 url 의 페이지 번호가 1번부터 22번까지 변경되면서 날짜와 게시글을 가져올 수 있도록 코드를 수정하시오 !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhAYXmrxc7jB"
      },
      "source": [
        "# 1. 웹에서 html 문서를 가져와 beautifulsoup 으로 파싱\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "for i in range(1, 23):\r\n",
        "    list_url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page='+ str(i) +'&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&'\r\n",
        "\r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "    \r\n",
        "\r\n",
        "    # 2. 시청자 게시판의 날짜와 본문 내용을 가져옵니다.\r\n",
        "    \r\n",
        "    result1 = soup.find_all( 'span', class_='date')\r\n",
        "    result2 = soup.find_all( 'p', class_='con')\r\n",
        "    \r\n",
        "\r\n",
        "    # 3. 시청자 게시판의 날짜와 본문을 params 와 params2 리스트에 담습니다.\r\n",
        "    \r\n",
        "    params1 = []\r\n",
        "    params2 = []\r\n",
        "    for i in result1:\r\n",
        "        params1.append( i.get_text(\" \", strip = True) )\r\n",
        "    for i in result2:\r\n",
        "        params2.append( i.get_text(\" \", strip = True) )\r\n",
        "    \r\n",
        "    \r\n",
        "    # 4. 날짜와 본문을 같이 출력합니다.\r\n",
        "    \r\n",
        "    for k, h in zip(params1, params2):\r\n",
        "        print( k + '   ' + h )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lK3WT3tdJNT"
      },
      "source": [
        "설명: 위의 params1 과 params2 리스트가 for 문 안쪽에 있기 때문에 페이지 번호가 바뀔때 마다 params 리스트 안의 내용이 새로운 날짜와 내용으로 변경되게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II-JLqTHdMVh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUZQIxPmdMQz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl2teTDGdMBj"
      },
      "source": [
        "예제13. params1 과 params2 에 레이디 버그 시청자 게시판의 모든 게시날짜와 본문내용이 들어가게 코드를 수정하시오 !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrcG7uhDdOKA"
      },
      "source": [
        "# 0. 웹스크롤링에 필요한 모듈을 import 합니다.\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "# 1. 레이디 버그 게시판 게시날짜와 게시글 전체를 다 담을 리스트 2개 생성\r\n",
        "params1 = []   # 게시 날짜\r\n",
        "params2 = []   # 게시글 본문\r\n",
        "\r\n",
        "for i in range(1, 23):\r\n",
        "\r\n",
        "    # 2. 웹에서 html 문서를 가져와 beautifulsoup 으로 파싱\r\n",
        "    list_url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page='+ str(i) +'&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&'\r\n",
        "    url = urllib.request.Request(list_url)    # 사람이 알아볼 수 있는 위의 url 을 파이썬이 \r\n",
        "\t\t\t\t\t\t\t    # 알아볼 수 있도록 변환 첫번째 작업\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")   # 두번째 작업\r\n",
        "                                                    # 위의 url 의 html 문서들을 result 변수에 담았다.\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "    \r\n",
        "    \r\n",
        "    # 3. 시청자 게시판의 날짜와 본문 내용을 가져옵니다.\r\n",
        "    \r\n",
        "    result1 = soup.find_all( 'span', class_='date')\r\n",
        "    result2 = soup.find_all( 'p', class_='con')\r\n",
        "    \r\n",
        "\r\n",
        "    # 4. 시청자 게시판의 날짜와 본문을 params 와 params2 리스트에 담습니다.\r\n",
        "    # 아까는 이자리에 params1 = [] , params2 = [] 가 있어어 페이지 번호가 변경될 때 마다\r\n",
        "    # params 리스트가 초기화가 되었었는데 지금은 이자리에 없고 맨 위에 for 문 시작하기 전에\r\n",
        "    # 있었으므로 계속 페이지번호 1~22 번까지 모든 게시날짜와 게시본문이 params1 과 \r\n",
        "    # params2에 append 됩니다.\r\n",
        "\r\n",
        "    for i in result1:\r\n",
        "        params1.append( i.get_text(\" \", strip = True) )\r\n",
        "    for i in result2:\r\n",
        "        params2.append( i.get_text(\" \", strip = True) )\r\n",
        "    \r\n",
        "\r\n",
        "# 5. 날짜와 본문을 같이 출력합니다.\r\n",
        "    \r\n",
        "for k, h in zip(params1, params2):\r\n",
        "    print( k + '   ' + h )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQcWo3gOdmlD"
      },
      "source": [
        "설명: 예제12번은 레이디 버그 게시판 전체글을 확면에 출력하는 코드이고 예제13번은 레이디 버그 게시판 전체글을 prams1 과 params2 리스트에 모두 담고 화면에 출력하는 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ9gfYfhdoqk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS0jSsPBdpSD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toii5paNdpAL"
      },
      "source": [
        "문제415. 레이디 버그 게시판의 전체 게시글을 총 몇건인가 ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXp_23jPdwjj",
        "outputId": "e89c6541-5e2a-4c2e-a529-785b02c629b3"
      },
      "source": [
        "# 1. 웹에서 html 문서를 가져와 beautifulsoup 으로 파싱\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "cnt = 0\r\n",
        "\r\n",
        "for i in range(1, 23):\r\n",
        "    \r\n",
        "    # 2. 웹에서 html 문서를 가져와 beautifulsoup 으로 파싱\r\n",
        "    list_url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page='+ str(i) +'&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&'\r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "    \r\n",
        "    # 3. 시청자 게시판의 날짜를 가져옵니다.\r\n",
        "    result1 = soup.find_all( 'span', class_='date')\r\n",
        "\r\n",
        "    # 4. 날짜의 건수를 cnt 변수로 할당하여 집계\r\n",
        "    for i in result1:\r\n",
        "        cnt += 1\r\n",
        "\r\n",
        "print(cnt)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa4dH829dtTM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3t2a8VHeAFc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBU0hRsueAAm"
      },
      "source": [
        "문제416. 게시글 431개 전체를 mytext3.txt 라는 이름으로 저장하시오"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAB45ZVneFn0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXBE-xbeDOl"
      },
      "source": [
        "** 텍스트파일을 저장하는 파이썬 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enNNNgi7eDj0"
      },
      "source": [
        "text = 'abcdefghijklmn'\r\n",
        "\r\n",
        "f = open(\"c:\\\\data\\\\mytext3.txt\", \"w\", encoding = \"UTF8\")\r\n",
        "f.write( text )\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJOvZyw7eHJl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhM3qh89eHCH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R3tnyDfeIdb"
      },
      "source": [
        "답:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT2we_xreNWL"
      },
      "source": [
        "# 0. 웹스크롤링에 필요한 모듈을 import 합니다.\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "# 1. 레이디 버그 게시판 게시날짜와 게시글 전체를 다 담을 리스트 2개 생성\r\n",
        "params1 = []   # 게시 날짜\r\n",
        "params2 = []   # 게시글 본문\r\n",
        "\r\n",
        "f = open(\"/content/drive/MyDrive/data/mytext3.txt\", \"w\", encoding = \"UTF8\")\r\n",
        "\r\n",
        "for i in range(1, 23):\r\n",
        "\r\n",
        "    # 2. 웹에서 html 문서를 가져와 beautifulsoup 으로 파싱\r\n",
        "    list_url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page='+ str(i) +'&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&'\r\n",
        "    url = urllib.request.Request(list_url)    # 사람이 알아볼 수 있는 위의 url 을 파이썬이 \r\n",
        "\t\t\t\t\t\t\t    # 알아볼 수 있도록 변환 첫번째 작업\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")   # 두번째 작업\r\n",
        "                                                    # 위의 url 의 html 문서들을 result 변수에 담았다.\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "    \r\n",
        "    # 3. 시청자 게시판의 날짜와 본문 내용을 가져옵니다.\r\n",
        "    \r\n",
        "    result1 = soup.find_all( 'span', class_='date')\r\n",
        "    result2 = soup.find_all( 'p', class_='con')\r\n",
        "    \r\n",
        "    # 4. 시청자 게시판의 날짜와 본문을 params 와 params2 리스트에 담습니다.\r\n",
        "    # 아까는 이자리에 params1 = [] , params2 = [] 가 있어어 페이지 번호가 변경될 때 마다\r\n",
        "    # params 리스트가 초기화가 되었었는데 지금은 이자리에 없고 맨 위에 for 문 시작하기 전에\r\n",
        "    # 있었으므로 계속 페이지번호 1~22 번까지 모든 게시날짜와 게시본문이 params1 과 \r\n",
        "    # params2에 append 됩니다.\r\n",
        "\r\n",
        "    for i in result1:\r\n",
        "        params1.append( i.get_text(\" \", strip = True) )\r\n",
        "    for i in result2:\r\n",
        "        params2.append( i.get_text(\" \", strip = True) )\r\n",
        "    \r\n",
        "\r\n",
        "# 5. 날짜와 본문을 같이 출력합니다.\r\n",
        "    \r\n",
        "for k, h in zip(params1, params2):\r\n",
        "    f.write( k + '   ' + h + '\\n' )\r\n",
        "\r\n",
        "f.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SorF0HzeG7M"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDQIReVeesF0"
      },
      "source": [
        "# mytext3.txt 데이터 확인\r\n",
        "f = open(\"/content/drive/MyDrive/data/mytext3.txt\", encoding = \"UTF8\")\r\n",
        "data = f.read() \r\n",
        "print(data)\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb1YRx0ZeyR6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKFVB6QkeyLn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g425_BQnex73"
      },
      "source": [
        "**■ 143. 웹스크롤링 실전 2단계 (중앙일보사)**\r\n",
        "\r\n",
        "중앙일보사 홈페이지에서 인공지능으로 검색을 했을때 나오는 기사들을 전부 웹스크롤링 한다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ8aM3mhou9b"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aba2geUovRG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtFZrZ9povh0"
      },
      "source": [
        "예제1. 중앙일보사 홈페이지에서 인공지능으로 검색했을 때 나오는 url 을 가져오시오 !\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYsIk8oCo3PU"
      },
      "source": [
        "https://news.joins.com/Search/JoongangNews?page=1&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkiQvwZIo55C"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMU6L4K7o6Fj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bt4I-T_o6XL"
      },
      "source": [
        "예제2. 위의 사이트에서 보이는 상세기사를 클릭하고 그 기사의 url 을 복사하시오 !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcnQUfFQpAEp"
      },
      "source": [
        "https://news.joins.com/article/23990482  \r\n",
        "https://news.joins.com/article/23990187  \r\n",
        "https://news.joins.com/article/23990176"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE3FoBDFpAzk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr1ylrrIpEEi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mRLxx95pD2C"
      },
      "source": [
        "예제3. 인공지능으로 검색했을때 나오는 상세기사들의 url 을 스크롤링 하시오 !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ5HG1hrpFrK"
      },
      "source": [
        "상세기사 url html\r\n",
        "\r\n",
        "    <h2 class=\"headline mg\">\r\n",
        "    <a href=\"https://news.joins.com/article/23947044\" target=\"_blank\">'AI 발전에 써라' 동원 김재철 명예회장, KAIST에 500억 기부</a>\r\n",
        "    </h2>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyDlFRhdpL-m",
        "outputId": "e3f1c179-32f4-478a-e0ab-461598824fc7"
      },
      "source": [
        "# 답:\r\n",
        "# 1. 웹스크롤링에 필요한 모듈을 import 합니다.\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "# 2. 중앙일보에서 인공지능으로 검색했을 때 나오는 첫페이지의 html 코드를\r\n",
        "#   beautifulsoup 에서 이용할 수 있도록 파싱\r\n",
        "list_url = 'https://news.joins.com/Search/JoongangNews?page=1&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews'\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "# print(soup)\r\n",
        "\r\n",
        "# 3. 상세기사 url 을 가져올 수 있도록 태그와 클래스를 찾습니다.\r\n",
        "#  찾아보니 태그는 h2 이고 클래스 이름은 headline mg 입니다.\r\n",
        "\r\n",
        "result1 = soup.find_all( 'h2', class_='headline mg')    # list 에 상세기사 html 리스트로 저장\r\n",
        "# print(result1)   \r\n",
        "\r\n",
        "# 4. 위의 result1 은 리스트 이므로 for loop 문을 이용해서 리스트에 있는 요소를 하나씩\r\n",
        "#   빼내면서 href 의 값을 얻어냅니다.\r\n",
        "\r\n",
        "for i in result1:  # result1 리스트의 요소를 하나씩 빼내는 코드\r\n",
        "    for k in i:     # h2 태그안의 a 태그의 html 코드를 가져오기 위한 코드. h2 태그 a 태그로 구분\r\n",
        "        print(k.get( 'href' ))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://news.joins.com/article/23990482\n",
            "https://news.joins.com/article/23990187\n",
            "https://news.joins.com/article/23990176\n",
            "https://news.joins.com/article/23990171\n",
            "https://news.joins.com/article/23989917\n",
            "https://news.joins.com/article/23989029\n",
            "https://news.joins.com/article/23988735\n",
            "https://news.joins.com/article/23988692\n",
            "https://news.joins.com/article/23988655\n",
            "https://news.joins.com/article/23988654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3o8EMmXpVvr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5CxZoyqpVrk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLbpOhbYpVnp"
      },
      "source": [
        "예제4. 위의상세 기사 url 을 params 라는 비어있는 리스트에 다 append 시키시오 !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mab3XONZpWxa",
        "outputId": "176445c5-beca-4b92-d108-6de268510a62"
      },
      "source": [
        "# 1. 웹스크롤링에 필요한 모듈을 import 합니다.\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "# 2. 중앙일보에서 인공지능으로 검색했을 때 나오는 첫페이지의 html 코드를\r\n",
        "#   beautifulsoup 에서 이용할 수 있도록 파싱\r\n",
        "list_url = 'https://news.joins.com/Search/JoongangNews?page=1&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews'\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "# print(soup)\r\n",
        "\r\n",
        "\r\n",
        "# 3. 상세기사 url 을 가져올 수 있도록 태그와 클래스를 찾습니다.\r\n",
        "#  찾아보니 태그는 h2 이고 클래스 이름은 headline mg 입니다.\r\n",
        "\r\n",
        "result1 = soup.find_all( 'h2', class_='headline mg')    # list 에 상세기사 html 리스트로 저장\r\n",
        "# print(result1)   \r\n",
        "\r\n",
        "\r\n",
        "# 4. 위의 result1 은 리스트 이므로 for loop 문을 이용해서 리스트에 있는 요소를 하나씩\r\n",
        "#   빼내면서 href 의 값을 얻어냅니다.\r\n",
        "\r\n",
        "params = []\r\n",
        "\r\n",
        "for i in result1:  # result1 리스트의 요소를 하나씩 빼내는 코드\r\n",
        "    for k in i:     # h2 태그안의 a 태그의 html 코드를 가져오기 위한 코드. h2 태그 a 태그로 구분\r\n",
        "        params.append(k.get( 'href' ))\r\n",
        "\r\n",
        "print(params)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://news.joins.com/article/23990482', 'https://news.joins.com/article/23990187', 'https://news.joins.com/article/23990176', 'https://news.joins.com/article/23990171', 'https://news.joins.com/article/23989917', 'https://news.joins.com/article/23989029', 'https://news.joins.com/article/23988735', 'https://news.joins.com/article/23988692', 'https://news.joins.com/article/23988655', 'https://news.joins.com/article/23988654']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjVcFz3ppdvL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJez_kUHpdsZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOt-zamgpdpm"
      },
      "source": [
        "예제5. 상세기사 url 중에 하나를 복사해오고 그 상세기사 url 의 웹페이지로 접속해서 본문 기사의 태그 이름과 클래스 이름이 무엇인지 확인하시오 !\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuaI0f2QpfRa"
      },
      "source": [
        "    <div id=\"article_body\" itemprop=\"articleBody\" class=\"article_body mg fs4\">\r\n",
        "\r\n",
        "답: 태그 이름은 div 태그 이고 클래스 이름은 \"article_body mg fs4\" 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ZYiOExphZW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpBnqOqKphqS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s45bR0MQph6Z"
      },
      "source": [
        "예제6. 위의 상세기사의 본문 텍스트를 스크롤링해서 출력하시오 !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LCLlMcYpjrI",
        "outputId": "30d78c83-7b2a-4507-89e2-0affffdb9f63"
      },
      "source": [
        "# 1. 웹스크롤링에 필요한 모듈을 import 합니다.\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "# 2. 상세기사 url 로 검색했을때 나오는 페이지의 html 코드를\r\n",
        "#   beautifulsoup 에서 이용할 수 있도록 파싱\r\n",
        "\r\n",
        "list_url = 'https://news.joins.com/article/23990482'\r\n",
        "url = urllib.request.Request(list_url)\r\n",
        "result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "# 3. 상세기사의 본문 을 가져올 수 있도록 태그와 클래스를 찾습니다.\r\n",
        "#  찾아보니 태그는 div 이고 클래스 이름은 article_body mg fs4 입니다.\r\n",
        "\r\n",
        "result1 = soup.find_all( 'div', class_='article_body mg fs4')\r\n",
        "\r\n",
        "# 4. 위의 result1 은 리스트 이므로 for loop 문을 이용해서 리스트에 있는 요소를 \r\n",
        "#   하나씩 빼내면서 본문의 텍스트를 얻어냅니다.\r\n",
        "\r\n",
        "params2 = []\r\n",
        "\r\n",
        "for i in result1:  # result1 리스트의 요소를 하나씩 빼내는 코드\r\n",
        "    params2.append( i.get_text(\" \", strip = True) )\r\n",
        "\r\n",
        "print(params2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['안상일 하이퍼커넥트 대표 미국 나스닥 상장사 매치그룹(시가총액 47조원)이 한국 스타트업 하이퍼커넥트를 2조원에 인수한다. 한국 스타트업 중에선 2019년 독일 딜리버리히어로(DH)가 4조7500억원에 인수한 우아한 형제들(배달의 민족)에 이어 두 번째로 큰 규모다. 안상일·정강식·용현택 공동창업 배민 M&A 이어 벤처 최대 규모 230개국 19개 언어, 5.4억건 다운 매년 매출 60%이상 초고속 성장 하이퍼커넥트는 “미국 매치그룹이 하이퍼커넥트 지분 100%를 17억2500만달러(1조9330억원)에 인수하기로 합의했다”고 10일 밝혔다. 동영상 채팅앱 ‘아자르’ ‘하쿠나라이브’ 운영사인 하이퍼커넥트는 2014년 설립된 스타트업이다. 전세계 새로운 사람과 만난다는 ‘소셜디스커버리’ 개념을 앞세워 글로벌 시장을 개척해왔다. 현재 230개국 19개 언어로 서비스 중이며 아자르의 누적 다운로드 건수는 5억4000만회에 달한다. 영상 메신저 앱 ‘아자르’는. 그래픽=신재민 기자 shin.jaemin@joongang.co.kr 비상장 스타트업으로는 드물게 적자 없이 매년 60% 이상의 매출이 늘고 있는 회사다. 2019년 매출은 1689억원, 영업이익은 202억원이다. 지난해는 상반기까지 1235억원을 벌었다. 동종업계에선 비디오 커뮤니케이션과 인공지능(AI) 분야에서 세계수준의 기술을 갖췄다고 평가한다. 벤처투자업계 한 관계자는 “국내 스타트업도 글로벌 무대에서도 충분히 통할 수 있다는 걸 보여준 성공 사례”라고 말했다. 하이퍼커넥트의 핵심 경쟁력은 영상 채팅 서비스 ‘아자르(Azar)’다. 원하는 상대의 지역과 성별을 택한 후 화면을 가로로 넘기면(스와이프) 무작위로 전 세계 가입자와 영상 대화를 할 수 있다. 이용자 99%가 해외 사용자로 지난해 12월 애플 앱스토어 전 세계 60개국 매출 탑 10(SNS부분 매출 기준)에 들었다. 하이퍼커넥트 매출. 그래픽=신재민 기자 shin.jaemin@joongang.co.kr 아자르는 매치그룹 주력사업인 소셜 데이팅 앱 ‘틴더’ 등과 궁합이 잘 맞는다는 평가다. 매치그룹은 40여 개 글로벌 소셜앱을 보유했지만 ‘틱톡’ 같은 영상 킬러앱이 없었다. 샤르 듀베이 매치그룹 최고경영자(CEO)는 “하이퍼커넥트의 라이브 영상·오디오 기술은 강력한 연결 수단”이라며 “하이퍼커넥트의 혁신 기술을 매치그룹 서비스에 적용하고 기술 투자도 적극 지원할 것”이라고 말했다. 지난해 초 하이퍼커넥트는 김상헌 전 네이버 대표를 고문으로 영입하고 투자 유치를 준비했다. 김 고문은 라인(LINE)을 일본에 상장시킨 주역이다. 하이퍼커넥트도 3000억~4000억원대 투자를 유치해 기업공개(IPO)하는 큰 그림을 그렸다. 이후 매치그룹이 적극적 투자 의사를 보였다. 하이퍼커넥트 관계자는 “수년 전부터 매치그룹이 관심을 보여왔고, 이번 투자제안도 가장 적극적으로 해왔다”며 “인수 이후에도 독립 경영 체제를 유지하는 방식을 제안해 논의가 급물살을 탔다”고 설명했다. 하이퍼커넥트는 안상일 대표(40)와 정강식 최고기술책임자(CTO), 용현택 최고연구책임자(CRO)가 공동창업한 회사다. 지분율은 공개되지 않았지만 2014년~2015년 초기 투자 이후 대규모 투자를 유치한 적이 없어 상당수 지분을 공동창업자가 보유한 것으로 추정된다. 안 대표와 정 CTO는 서울대 대학 창업동아리 친구 사이기도 하다. 이번 인수로 초기 투자사인 알토스벤처스와 소프트뱅크벤처스도 잭팟을 터뜨렸다. 알토스벤처스는 2014년 22억원을 투자했고, 이듬해 소프트뱅크벤처스와 함께 100억원을 추가 투자했다. 두 투자사가 보유한 하이퍼커넥트 주식은 370만주 가량(우선주 포함)으로 수천억원대 이익을 낼 것으로 예상한다. 정원엽·김정민 기자\\xa0jung.wonyeob@joongang.co.kr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJO1RW6tpumr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tEMe99HpujG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAcRqx11pufa"
      },
      "source": [
        "예제7. 상세기사 url 을 가져와서 params 리스트에 넣고 출력하는 예제4번 코드를 함수로 생성하시오 !\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVxsqWyWpvvC",
        "outputId": "97047552-f9b3-4fd4-f577-4bf103bbd6c0"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "def j_scroll():\r\n",
        "    list_url = 'https://news.joins.com/Search/JoongangNews?page=1&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews'\r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "    result1 = soup.find_all( 'h2', class_='headline mg')\r\n",
        "    \r\n",
        "    params = []\r\n",
        "    \r\n",
        "    for i in result1:  \r\n",
        "        for k in i:\r\n",
        "            params.append(k.get( 'href' ))\r\n",
        "\r\n",
        "    return params\r\n",
        "\r\n",
        "print( j_scroll() )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://news.joins.com/article/23990482', 'https://news.joins.com/article/23990187', 'https://news.joins.com/article/23990176', 'https://news.joins.com/article/23990171', 'https://news.joins.com/article/23989917', 'https://news.joins.com/article/23989029', 'https://news.joins.com/article/23988735', 'https://news.joins.com/article/23988692', 'https://news.joins.com/article/23988655', 'https://news.joins.com/article/23988654']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKmwiPwcpviy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCGJ_bkZp0_6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfQPTeQ5p08K"
      },
      "source": [
        "예제8. 상세 기사 url 로 본문 기사를 스크롤링해서 리스트에 담았던 예제6번을 \r\n",
        "\t   j_detail_scroll() 함수로 생성하시오 !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZoeT9tHp2uS",
        "outputId": "e50e0e6a-6563-4fe8-ca48-21e56a69450c"
      },
      "source": [
        "from  bs4  import  BeautifulSoup\r\n",
        "import  urllib.request   \r\n",
        "\r\n",
        "def j_detail_scroll():\r\n",
        "    list_url = 'https://news.joins.com/article/23990482'\r\n",
        "    url = urllib.request.Request(list_url) \r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") \r\n",
        "    soup = BeautifulSoup( result, \"html.parser\")\r\n",
        "            \r\n",
        "    result1 = soup.find_all( 'div', class_ = 'article_body mg fs4')\r\n",
        "    \r\n",
        "    params2 =[ ]\r\n",
        "    \r\n",
        "    for  i  in  result1:\r\n",
        "        params2.append( i.get_text(\" \", strip=True) )  \r\n",
        "    \r\n",
        "    return params2\r\n",
        "\r\n",
        "print( j_detail_scroll() )"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['안상일 하이퍼커넥트 대표 미국 나스닥 상장사 매치그룹(시가총액 47조원)이 한국 스타트업 하이퍼커넥트를 2조원에 인수한다. 한국 스타트업 중에선 2019년 독일 딜리버리히어로(DH)가 4조7500억원에 인수한 우아한 형제들(배달의 민족)에 이어 두 번째로 큰 규모다. 안상일·정강식·용현택 공동창업 배민 M&A 이어 벤처 최대 규모 230개국 19개 언어, 5.4억건 다운 매년 매출 60%이상 초고속 성장 하이퍼커넥트는 “미국 매치그룹이 하이퍼커넥트 지분 100%를 17억2500만달러(1조9330억원)에 인수하기로 합의했다”고 10일 밝혔다. 동영상 채팅앱 ‘아자르’ ‘하쿠나라이브’ 운영사인 하이퍼커넥트는 2014년 설립된 스타트업이다. 전세계 새로운 사람과 만난다는 ‘소셜디스커버리’ 개념을 앞세워 글로벌 시장을 개척해왔다. 현재 230개국 19개 언어로 서비스 중이며 아자르의 누적 다운로드 건수는 5억4000만회에 달한다. 영상 메신저 앱 ‘아자르’는. 그래픽=신재민 기자 shin.jaemin@joongang.co.kr 비상장 스타트업으로는 드물게 적자 없이 매년 60% 이상의 매출이 늘고 있는 회사다. 2019년 매출은 1689억원, 영업이익은 202억원이다. 지난해는 상반기까지 1235억원을 벌었다. 동종업계에선 비디오 커뮤니케이션과 인공지능(AI) 분야에서 세계수준의 기술을 갖췄다고 평가한다. 벤처투자업계 한 관계자는 “국내 스타트업도 글로벌 무대에서도 충분히 통할 수 있다는 걸 보여준 성공 사례”라고 말했다. 하이퍼커넥트의 핵심 경쟁력은 영상 채팅 서비스 ‘아자르(Azar)’다. 원하는 상대의 지역과 성별을 택한 후 화면을 가로로 넘기면(스와이프) 무작위로 전 세계 가입자와 영상 대화를 할 수 있다. 이용자 99%가 해외 사용자로 지난해 12월 애플 앱스토어 전 세계 60개국 매출 탑 10(SNS부분 매출 기준)에 들었다. 하이퍼커넥트 매출. 그래픽=신재민 기자 shin.jaemin@joongang.co.kr 아자르는 매치그룹 주력사업인 소셜 데이팅 앱 ‘틴더’ 등과 궁합이 잘 맞는다는 평가다. 매치그룹은 40여 개 글로벌 소셜앱을 보유했지만 ‘틱톡’ 같은 영상 킬러앱이 없었다. 샤르 듀베이 매치그룹 최고경영자(CEO)는 “하이퍼커넥트의 라이브 영상·오디오 기술은 강력한 연결 수단”이라며 “하이퍼커넥트의 혁신 기술을 매치그룹 서비스에 적용하고 기술 투자도 적극 지원할 것”이라고 말했다. 지난해 초 하이퍼커넥트는 김상헌 전 네이버 대표를 고문으로 영입하고 투자 유치를 준비했다. 김 고문은 라인(LINE)을 일본에 상장시킨 주역이다. 하이퍼커넥트도 3000억~4000억원대 투자를 유치해 기업공개(IPO)하는 큰 그림을 그렸다. 이후 매치그룹이 적극적 투자 의사를 보였다. 하이퍼커넥트 관계자는 “수년 전부터 매치그룹이 관심을 보여왔고, 이번 투자제안도 가장 적극적으로 해왔다”며 “인수 이후에도 독립 경영 체제를 유지하는 방식을 제안해 논의가 급물살을 탔다”고 설명했다. 하이퍼커넥트는 안상일 대표(40)와 정강식 최고기술책임자(CTO), 용현택 최고연구책임자(CRO)가 공동창업한 회사다. 지분율은 공개되지 않았지만 2014년~2015년 초기 투자 이후 대규모 투자를 유치한 적이 없어 상당수 지분을 공동창업자가 보유한 것으로 추정된다. 안 대표와 정 CTO는 서울대 대학 창업동아리 친구 사이기도 하다. 이번 인수로 초기 투자사인 알토스벤처스와 소프트뱅크벤처스도 잭팟을 터뜨렸다. 알토스벤처스는 2014년 22억원을 투자했고, 이듬해 소프트뱅크벤처스와 함께 100억원을 추가 투자했다. 두 투자사가 보유한 하이퍼커넥트 주식은 370만주 가량(우선주 포함)으로 수천억원대 이익을 낼 것으로 예상한다. 정원엽·김정민 기자\\xa0jung.wonyeob@joongang.co.kr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z81EQz6p5O9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3eeZbWZp5Ia"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okBSxz0wp4rW"
      },
      "source": [
        "예제9. 지금 만든 j_detail_scroll() 함수는 상세기사 딱 한개의 본문을 출력하는 함수인데\r\n",
        "\t  이 j_detail_scroll() 함수에 j_scroll() 함수를 실행했을때 나오는 상세 url 여러개를\r\n",
        "\t  제공할 수 있도록 코드를 수정하시오!\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfoX2n20qBei"
      },
      "source": [
        "from  bs4  import  BeautifulSoup\r\n",
        "import  urllib.request\r\n",
        "\r\n",
        "def j_scroll():    # 상세기사 url 가져오는 함수\r\n",
        "    list_url = 'https://news.joins.com/Search/JoongangNews?page=1&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews'\r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "    result1 = soup.find_all( 'h2', class_='headline mg')\r\n",
        "    \r\n",
        "    params = []\r\n",
        "    \r\n",
        "    for i in result1:  \r\n",
        "        for k in i:\r\n",
        "            params.append(k.get( 'href' ))\r\n",
        "\r\n",
        "    return params\r\n",
        "\r\n",
        "\r\n",
        "def j_detail_scroll():    # 상세기사 url 하나 중에 본문내용 가져오는 함수\r\n",
        "    list_url = j_scroll()        # j_scroll 에 params 리스트가 할당\r\n",
        "    params2 =[ ]\r\n",
        "    for i in list_url:            # for loop 문으로 위의 리스트의 값을 하나씩 가져온다\r\n",
        "        url = urllib.request.Request(i) \r\n",
        "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") \r\n",
        "        soup = BeautifulSoup( result, \"html.parser\")\r\n",
        "                \r\n",
        "        result1 = soup.find_all( 'div', class_ = 'article_body mg fs4') \r\n",
        "        \r\n",
        "        for  i  in  result1:\r\n",
        "            params2.append( i.get_text(\" \", strip=True) )  \r\n",
        "        \r\n",
        "        return params2"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHj622deqGpW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YICUVvSqGlF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_HSiSjiqGhf"
      },
      "source": [
        "**■ 중앙일보의 기사를 스크롤링하는 내용 총정리**\r\n",
        "\r\n",
        "    1. 검색 키워드(예: 인공지능) 를 가지고 검색을 한 후 그 url 을 얻어낸다.\r\n",
        "    2. 상세기사 url 을 리스트에 담는 j_scroll() 함수를 생성한다.\r\n",
        "    3. 상세기사 url 로 기사본문을 스크롤링하는 j_detail_scroll() 함수를 생성한다.\r\n",
        "    4. j_detail_scroll() 함수 안에서 j_scroll() 함수를 호출해서 상세기사 url 여러개를 받아오도록 코딩하고 받아온 상세기사 url 로 본문 기사들을 params2 에 담도록 코드를 수정한다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMjvEMboqNWC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g1HM0vMqOwy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN6do3XMqOi9"
      },
      "source": [
        "문제417. 동아일보에서 검색 키워드(예: 인공지능) 를 가지고 검색을 한 후 그 url 을 얻어낸다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSI1u1TQqSHS"
      },
      "source": [
        "https://www.donga.com/news/search?p=1&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\r\n",
        "\r\n",
        "https://www.donga.com/news/search?p=16&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\r\n",
        "\r\n",
        "https://www.donga.com/news/search?p=31&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wHW5kLmqTby"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipKhlIBJqTTK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1EhkRr1qS-1"
      },
      "source": [
        "문제418. 상세기사 url 을 리스트에 담는 j_scroll() 함수를 생성한다.\r\n",
        "\r\n",
        "    <p class=\"txt\"><a href=\"https://www.donga.com/news/article/all/20201216/104475349/1\" target=\"_blank\">동원그룹 명예회장이 국내 <span class=\"highlight\">인공지능</span>(AI) 분야 인재 육성과 기술 발전을 위해 써달라며 카이스트(KAIST)에 사재 500억 원을 기부했다.동원그룹은 김재철 명예회장이 16일 카이스트 대전 본원 ...</a></p>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuUD7r-vqR7i",
        "outputId": "b3e7c260-7a83-4d01-d4c2-f5110f51eae2"
      },
      "source": [
        "from  bs4  import  BeautifulSoup\r\n",
        "import  urllib.request\r\n",
        "\r\n",
        "def d_scroll():    # 상세기사 url 가져오는 함수\r\n",
        "    list_url = 'https://www.donga.com/news/search?p=1&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'\r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "    result1 = soup.find_all( 'p', class_='txt')\r\n",
        "    \r\n",
        "    params = []\r\n",
        "    \r\n",
        "    for i in result1:  \r\n",
        "        for k in i:\r\n",
        "            params.append(k.get( 'href' ))\r\n",
        "            \r\n",
        "    return params\r\n",
        "\r\n",
        "print( d_scroll() )"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://www.donga.com/news/article/all/20210211/105380070/1', 'https://www.donga.com/news/article/all/20210211/105380055/1', 'https://www.donga.com/news/article/all/20210211/105380042/1', 'https://www.donga.com/news/article/all/20210210/105378590/1', 'https://www.donga.com/news/article/all/20210210/105376919/1', 'https://www.donga.com/news/article/all/20210210/105375958/1', 'https://www.donga.com/news/article/all/20210210/105374789/1', 'https://www.donga.com/news/article/all/20210210/105372049/2', 'https://www.donga.com/news/article/all/20210210/105371940/1', 'https://www.donga.com/news/article/all/20210210/105370266/2', 'https://www.donga.com/news/article/all/20210210/105366959/2', 'https://www.donga.com/news/article/all/20210209/105364082/1', 'https://www.donga.com/news/article/all/20210210/105364582/1', 'https://www.donga.com/news/article/all/20210210/105364567/1', 'https://www.donga.com/news/article/all/20210209/105363914/1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmTMfTtJqaqF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxJooGv2qake"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT34xB-eqaeC"
      },
      "source": [
        "문제419. 상세기사 url 로 기사본문을 스크롤링하는 j_detail_scroll() 함수를 생성한다.\r\n",
        "\r\n",
        "    <div class=\"article_txt\" style=\"font-size: 18px;\">\r\n",
        "            <strong class=\"sub_title\">김 명예회장 “카이스트 국내 AI 개발 속도 촉진 역할 기대”<br>\r\n",
        "    카이스트, ‘김재철 AI대학원’ 서울 이전 추진<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJKozbB-qPpK"
      },
      "source": [
        "# 답:\r\n",
        "from  bs4  import  BeautifulSoup\r\n",
        "import  urllib.request\r\n",
        "\r\n",
        "def d_detail_scroll():    # 상세기사 url 하나 중에 본문내용 가져오는 함수\r\n",
        "    list_url = d_scroll()\r\n",
        "    params2 =[ ]\r\n",
        "    for i in list_url:\r\n",
        "        url = urllib.request.Request(i) \r\n",
        "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") \r\n",
        "        soup = BeautifulSoup( result, \"html.parser\")\r\n",
        "                \r\n",
        "        result1 = soup.find_all( 'div', class_ = 'article_txt') \r\n",
        "        \r\n",
        "        for  i  in  result1:\r\n",
        "            params2.append( i.get_text(\" \", strip=True) )  \r\n",
        "        \r\n",
        "    return params2\r\n",
        "\r\n",
        "\r\n",
        "print( d_detail_scroll() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYwJ7eCHqqam"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIWmRqVVqqQN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i9BvWBdqqCa"
      },
      "source": [
        "문제420. 한겨례 신문사에서 인공지능으로 검색했을때 나오는 기사 본문을 스크롤링하는 함수 두개를 생성하시오 !\r\n",
        "\r\n",
        "    1. h_scroll()  : 상세기사 url 을 가져오는 함수 \r\n",
        "    2. h_detail_scroll()  : 상세기사 url 로 기사본문을 가져오는 함수 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iGTUGKRrNjm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ELgOzMdrNEu"
      },
      "source": [
        " 1. h_scroll() : 상세기사 url 을 가져오는 함수\r\n",
        "\r\n",
        " \r\n",
        "    <li class=\"first-child\">\r\n",
        "\t\t\t\t\t\t<dl>\r\n",
        "\t\t\t\t\t\t\t<dd class=\"photo\">\r\n",
        "\t\t\t\t\t\t\t\r\n",
        "\t\t\t\t\t\t\t</dd>\r\n",
        "\t\t\t\t\t\t\t<dt><a href=\"//www.hani.co.kr/arti/society/ngo/974369.html\">12월 16일 알림</a></dt>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_44iKREWqc62",
        "outputId": "138130e3-98bc-4a39-e8fd-d3492aa77055"
      },
      "source": [
        " #1. h_scroll() : 상세기사 url 을 가져오는 함수\r\n",
        "\r\n",
        "from  bs4  import  BeautifulSoup\r\n",
        "import  urllib.request\r\n",
        "\r\n",
        "def h_scroll():    # 상세기사 url 가져오는 함수\r\n",
        "    list_url = 'http://search.hani.co.kr/Search?command=query&keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&media=news&submedia=&sort=d&period=all&datefrom=1988.01.01&dateto=2020.12.16&pageseq=0'    \r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "    result1 = soup.find_all( 'dt' )\r\n",
        "    \r\n",
        "    params = []\r\n",
        "    \r\n",
        "    for i in result1:  \r\n",
        "        for k in i:\r\n",
        "            params.append( 'http:' + k.get( 'href' ))\r\n",
        "            \r\n",
        "    return params\r\n",
        "\r\n",
        "print( h_scroll() )"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['http://www.hani.co.kr/arti/economy/marketing/974581.html', 'http://www.hani.co.kr/arti/society/society_general/974543.html', 'http://www.hani.co.kr/arti/society/ngo/974369.html', 'http://www.hani.co.kr/arti/opinion/column/974362.html', 'http://www.hani.co.kr/arti/society/media/974346.html', 'http://www.hani.co.kr/arti/economy/economy_general/974292.html', 'http://www.hani.co.kr/arti/economy/economy_general/974274.html', 'http://www.hani.co.kr/arti/international/international_general/974227.html', 'http://www.hani.co.kr/arti/society/ngo/974199.html', 'http://www.hani.co.kr/arti/economy/economy_general/974056.html']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ7YZrO2rFKy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcI_D1gPrI_y"
      },
      "source": [
        " 2. h_detail_scroll() : 상세기사 url 로 기사본문을 가져오는 함수\r\n",
        "\r\n",
        " \r\n",
        "    <div class=\"text\">◇ 육군은 15일 국립대전현충에서 ‘6·25 전사자 발굴 유해 합동 안장식'을 엄수했다. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7bQPWc5rFa2"
      },
      "source": [
        "def h_detail_scroll():    # 상세기사 url 하나 중에 본문내용 가져오는 함수\r\n",
        "    list_url = h_scroll()\r\n",
        "    params2 =[ ]\r\n",
        "    for i in list_url:\r\n",
        "        url = urllib.request.Request(i) \r\n",
        "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") \r\n",
        "        soup = BeautifulSoup( result, \"html.parser\")\r\n",
        "                \r\n",
        "        result1 = soup.find_all( 'div', class_ = 'text') \r\n",
        "        \r\n",
        "        for  i  in  result1:\r\n",
        "            params2.append( i.get_text(\" \", strip=True) )  \r\n",
        "        \r\n",
        "    return params2\r\n",
        "\r\n",
        "\r\n",
        "print( h_detail_scroll() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Z6xUuKrYK6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-7-ySMurYGi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqUQYA7LrX_k"
      },
      "source": [
        "문제421. 원하는 신문사에서 여러분들이 스크롤링하고 싶은 키워드를 넣고 검색했을때 나오는 본문기사들을 수집하는 함수 2개를 생성하시오 ~ (밑에 페이지번호 1, 2, 3 만 스크롤링하세요)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs-EN5gmrfh4"
      },
      "source": [
        "국민일보\r\n",
        "\r\n",
        " 1. _scroll() : 상세기사 url 을 가져오는 함수\r\n",
        "\r\n",
        "\r\n",
        "    <dt class=\"tit\"><a href=\"http://news.kmib.co.kr/article/view.asp?arcid=0015330665\">스카이문스테크놀로지 SK텔레콤주식회사와 14억원 계약체결</a></dt>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX46bGT7rdNI",
        "outputId": "03b8100a-1546-4528-e5af-3e658bdcef9b"
      },
      "source": [
        "from  bs4  import  BeautifulSoup\r\n",
        "import  urllib.request\r\n",
        "\r\n",
        "def k_scroll():    # 상세기사 url 가져오는 함수\r\n",
        "    list_url = 'http://www.kmib.co.kr/search/searchResult.asp?searchWord=%uC778%uACF5%uC9C0%uB2A5&pageNo=1&period='    \r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"cp949\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "    result1 = soup.find_all( 'dt', class_ =\"tit\" )\r\n",
        "\r\n",
        "    params = []\r\n",
        "    \r\n",
        "    for i in result1:  \r\n",
        "        for k in i:\r\n",
        "            params.append(k.get( 'href' ))\r\n",
        "            \r\n",
        "    return params\r\n",
        "\r\n",
        "print( k_scroll() )"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['http://news.kmib.co.kr/article/view.asp?arcid=0015525530', 'http://news.kmib.co.kr/article/view.asp?arcid=0924178195', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524568', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524553', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524531', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524505', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524490', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524454', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524431', 'http://news.kmib.co.kr/article/view.asp?arcid=0015524393']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoasnbHfrnp2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEGo3URlrnci"
      },
      "source": [
        " 2. _detail_scroll() : 상세기사 url 로 기사본문을 가져오는 함수\r\n",
        "\r\n",
        "\r\n",
        "    <div class=\"tx\" id=\"articleBody\" itemprop=\"articleBody\">\r\n",
        "\t\t\t\r\n",
        "\t\t\t\t<!-- 기사 내용 -->\r\n",
        "\r\n",
        "\t\t\t\t\t<div align=\"center\"><figure style=\"display:table;\"><img width=\"100%\" alt=\"\" src=\"http://image.kmib.co.kr/online_image/2020/1216/611412110015330665_1.jpg\" style=\"display:block;\"></figure></div><br>스카이문스테크놀로지는 SK텔레콤주식회사와 14억원 규모의 계약을 체결했다고 16일에 공시\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU2N6jnqrqXa"
      },
      "source": [
        "def k_detail_scroll():    # 상세기사 url 하나 중에 본문내용 가져오는 함수\r\n",
        "    list_url = k_scroll()\r\n",
        "    params2 =[ ]\r\n",
        "    for i in list_url:\r\n",
        "        url = urllib.request.Request(i) \r\n",
        "        result = urllib.request.urlopen(url).read().decode(\"cp949\") \r\n",
        "        soup = BeautifulSoup( result, \"html.parser\")\r\n",
        "                \r\n",
        "        result1 = soup.find_all( 'div', class_ = \"tx\") \r\n",
        "        \r\n",
        "        for  i  in  result1:\r\n",
        "            params2.append( i.get_text(\" \", strip=True) )  \r\n",
        "        \r\n",
        "    return params2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei3vY7ilsAbK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FopUmcv2sA6y"
      },
      "source": [
        "# 답:\r\n",
        "from  bs4  import  BeautifulSoup\r\n",
        "import  urllib.request\r\n",
        "\r\n",
        "def k_scroll(num):    # 상세기사 url 가져오는 함수\r\n",
        "    list_url = 'http://www.kmib.co.kr/search/searchResult.asp?searchWord=%uC778%uACF5%uC9C0%uB2A5&pageNo='+str(num)+'&period='\r\n",
        "    url = urllib.request.Request(list_url)\r\n",
        "    result = urllib.request.urlopen(url).read().decode(\"cp949\")\r\n",
        "    soup = BeautifulSoup( result, \"html.parser\" )\r\n",
        "\r\n",
        "    result1 = soup.find_all( 'dt', class_ =\"tit\" )\r\n",
        "\r\n",
        "    params = []\r\n",
        "    \r\n",
        "    for i in result1:  \r\n",
        "        for k in i:\r\n",
        "            params.append(k.get( 'href' ))\r\n",
        "            \r\n",
        "    return params\r\n",
        "\r\n",
        "def k_detail_scroll(num):    # 상세기사 url 하나 중에 본문내용 가져오는 함수\r\n",
        "    list_url = k_scroll(num)\r\n",
        "    params2 =[ ]\r\n",
        "    for i in list_url:\r\n",
        "        url = urllib.request.Request(i) \r\n",
        "        result = urllib.request.urlopen(url).read().decode(\"cp949\") \r\n",
        "        soup = BeautifulSoup( result, \"html.parser\")\r\n",
        "                \r\n",
        "        result1 = soup.find_all( 'div', class_ = \"tx\") \r\n",
        "        \r\n",
        "        for  i  in  result1:\r\n",
        "            params2.append( i.get_text(\" \", strip=True) )  \r\n",
        "        \r\n",
        "    return params2\r\n",
        "\r\n",
        "\r\n",
        "for i in range(1,4):\r\n",
        "    print( k_detail_scroll(i) )    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}